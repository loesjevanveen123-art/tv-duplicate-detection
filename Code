############################################################
# FULLY AUTOMATIC F1-OPTIMIZED LSH + MSM PIPELINE
# Includes:
#  - Model-word extraction
#  - Cleaning
#  - MinHash 
#  - LSH banding
#  - MSM similarity
#  - FULL TUNING:
#       * LSH bands
#       * MSM threshold
#       * MSM weights (kV, HSM, TMW)
#  - Outputs 
############################################################

library(jsonlite)
library(stringr)
library(dplyr)
library(purrr)
library(tibble)

`%||%` <- function(a,b) if (is.null(a)) b else a


############################################################
# 1. LOAD DATA
############################################################

load_tv_data <- function(path){
  raw <- fromJSON(path, simplifyDataFrame = FALSE)
  out <- list(); k <- 1
  for(model in names(raw)){
    for(rec in raw[[model]]){
      out[[k]] <- list(
        id      = k,
        modelID = rec$modelID %||% NA_character_,
        title   = rec$title %||% "",
        features= rec$featuresMap %||% list()
      )
      k <- k + 1
    }
  }
  tibble(
    id=map_int(out,"id"),
    modelID=map_chr(out,"modelID"),
    title=map_chr(out,"title"),
    features=map(out,"features")
  )
}


############################################################
# 2. CLEANING
############################################################

normalize_units_string <- function(x){
  if(is.null(x)) return(NA_character_)
  x <- tolower(as.character(x))
  x <- str_replace_all(x,"\""," inch")
  x <- str_replace_all(x,"-inch"," inch")
  x <- str_replace_all(x,"inches"," inch")
  x <- str_replace_all(x,"hertz"," hz")
  x <- str_replace_all(x,"hz"," hz")
  str_squish(x)
}

clean_key <- function(x){
  y <- tolower(x)
  y <- gsub("[^a-z0-9 ]"," ",y)
  str_squish(y)
}

clean_tv_data <- function(df){
  df %>% mutate(
    title_clean = normalize_units_string(title),
    features_clean = map(features, ~ map_chr(.x, normalize_units_string)),
    features_keys_clean = map(features, function(f){
      if(length(f)==0) return(character(0))
      orig <- names(f)
      setNames(clean_key(orig), orig)
    })
  )
}


############################################################
# 3. MODEL WORDS (title + value)
############################################################

extract_model_words_title <- function(title){
  if(is.na(title) || title=="") return(character(0))
  t <- tolower(gsub("[^a-z0-9]"," ",title))
  tok <- unlist(strsplit(t,"\\s+"))
  tok <- tok[nchar(tok)>=4]
  tok <- tok[grepl("[a-z]",tok) & grepl("[0-9]",tok)]
  
  bad <- c("1080p","720p","led","lcd","tv","uhd","hdr","smart",
           "class","series","inch","hz","diag")
  tok <- setdiff(tok,bad)
  unique(tok)
}

extract_model_words_values <- function(values){
  if(length(values)==0) return(character(0))
  txt <- tolower(paste(values, collapse=" "))
  
  out <- c()
  out <- c(out, str_extract_all(txt,"\\b\\d{3,4}x\\d{3,4}\\b")[[1]])
  out <- c(out, str_extract_all(txt,"\\b(100|120|144|200|240) hz\\b")[[1]])
  
  unique(out)
}


############################################################
# 4. VOCAB + INDEXING
############################################################

build_vocabulary <- function(mw_sets){
  sort(unique(unlist(mw_sets)))
}

build_indexed_sets <- function(mw_sets, vocab){
  idmap <- seq_along(vocab); names(idmap)=vocab
  sets_idx <- map(mw_sets, ~ idmap[.x])
  list(vocab=vocab, sets_idx=sets_idx)
}


############################################################
# 5. MINHASH + LSH
############################################################

compute_minhash_signatures <- function(sets_idx, vocab_size,
                                       num_hashes=100, seed=1){
  set.seed(seed)
  p <- 1000003L
  a <- sample.int(p-1,num_hashes)
  b <- sample.int(p-1,num_hashes)
  N <- length(sets_idx)
  
  S <- matrix(Inf, num_hashes, N)
  
  for(j in 1:N){
    idx <- sets_idx[[j]]
    if(length(idx)==0) next
    S[,j] <- vapply(1:num_hashes, function(h){
      min((a[h] + b[h]*idx) %% p)
    }, numeric(1))
  }
  
  list(S=S)
}

lsh_candidates <- function(S, bands){
  num_hashes <- nrow(S)
  N <- ncol(S)
  r <- num_hashes / bands
  
  env <- new.env()
  
  for(b in 1:bands){
    r1 <- (b-1)*r+1
    r2 <- b*r
    block <- S[r1:r2,,drop=FALSE]
    
    bucket <- new.env()
    for(j in 1:N){
      key <- paste(block[,j],collapse="-")
      if(!exists(key,bucket)) bucket[[key]] <- j
      else bucket[[key]] <- c(bucket[[key]],j)
    }
    for(key in ls(bucket)){
      inds <- bucket[[key]]
      if(length(inds)<2) next
      cmb <- combn(inds,2)
      for(k in 1:ncol(cmb)){
        i <- min(cmb[,k]); j <- max(cmb[,k])
        env[[paste(i,j,sep="_")]] <- TRUE
      }
    }
  }
  keys <- ls(env)
  if(length(keys)==0) return(tibble(i=integer(0),j=integer(0)))
  ij <- str_split_fixed(keys,"_",2)
  tibble(i=as.integer(ij[,1]), j=as.integer(ij[,2]))
}


############################################################
# 6. MSM SIMILARITY
############################################################

jaccard_similarity <- function(a,b){
  if(length(a)==0 && length(b)==0) return(0)
  length(intersect(a,b))/length(unique(c(a,b)))
}

qgrams <- function(x,q=3){
  x <- tolower(as.character(x))
  x <- str_squish(x)
  if(nchar(x)<q) return(character(0))
  sapply(1:(nchar(x)-q+1), function(i) substr(x,i,i+q-1))
}

qgram_similarity <- function(a,b,q=3){
  qa <- unique(qgrams(a,q))
  qb <- unique(qgrams(b,q))
  if(length(qa)==0 && length(qb)==0) return(0)
  length(intersect(qa,qb)) / length(unique(c(qa,qb)))
}

msm_similarity_pair <- function(i,j,df,title_mw,value_mw,
                                w_kv,w_hsm,w_tmw){
  
  # key matching
  k1 <- df$features_keys_clean[[i]]
  k2 <- df$features_keys_clean[[j]]
  C_kv <- 0
  if(length(k1)>0 && length(k2)>0){
    c1 <- unname(k1)
    c2 <- unname(k2)
    com <- intersect(c1,c2)
    if(length(com)>0){
      sims <- numeric(length(com))
      oi <- names(k1); oj <- names(k2)
      for(k in seq_along(com)){
        ck <- com[k]
        v_i <- df$features_clean[[i]][[ oi[which(c1==ck)[1]] ]]
        v_j <- df$features_clean[[j]][[ oj[which(c2==ck)[1]] ]]
        sims[k] <- qgram_similarity(v_i,v_j)
      }
      C_kv <- mean(sims)
    }
  }
  
  C_hsm <- jaccard_similarity(value_mw[[i]], value_mw[[j]])
  C_tmw <- jaccard_similarity(title_mw[[i]], title_mw[[j]])
  
  w_kv*C_kv + w_hsm*C_hsm + w_tmw*C_tmw
}

predict_msm <- function(cp,df,title_mw,value_mw,
                        w_kv,w_hsm,w_tmw){
  if(nrow(cp)==0) return(cp %>% mutate(msm_score=numeric(0)))
  sc <- numeric(nrow(cp))
  for(k in 1:nrow(cp)){
    sc[k] <- msm_similarity_pair(cp$i[k],cp$j[k],
                                 df,title_mw,value_mw,
                                 w_kv,w_hsm,w_tmw)
  }
  cp %>% mutate(msm_score=sc)
}


############################################################
# 7. EVALUATION
############################################################

compute_gold_duplicate_pairs <- function(modelIDs){
  env <- new.env()
  N <- length(modelIDs)
  for(i in 1:(N-1)){
    for(j in (i+1):N){
      if(!is.na(modelIDs[i]) && modelIDs[i]==modelIDs[j])
        env[[paste(i,j,sep="_")]] <- TRUE
    }
  }
  keys <- ls(env)
  if(length(keys)==0) return(tibble(i=integer(0),j=integer(0)))
  ij <- str_split_fixed(keys,"_",2)
  tibble(i=as.integer(ij[,1]), j=as.integer(ij[,2]))
}

evaluate_final <- function(pred,gold,N){
  allp <- t(combn(N,2))
  allk <- paste(allp[,1],allp[,2],sep="_")
  goldk <- paste(gold$i,gold$j,sep="_")
  predk <- paste(pred$i[pred$pred_duplicate], pred$j[pred$pred_duplicate],sep="_")
  
  is_p <- allk %in% predk
  is_g <- allk %in% goldk
  
  TP <- sum(is_p & is_g)
  FP <- sum(is_p & !is_g)
  FN <- sum(!is_p & is_g)
  TN <- sum(!is_p & !is_g)
  
  prec <- TP/(TP+FP)
  rec  <- TP/(TP+FN)
  F1   <- ifelse(prec+rec==0,0,2*prec*rec/(prec+rec))
  
  list(TP=TP,FP=FP,FN=FN,TN=TN,precision=prec,recall=rec,F1=F1)
}


############################################################
# 8. FULL TUNING (bands, threshold, weights)
############################################################

tune_all <- function(df){
  N <- nrow(df)
  
  title_mw <- map(df$title_clean, extract_model_words_title)
  value_mw <- map(df$features_clean, extract_model_words_values)
  mw_sets <- map2(title_mw,value_mw, ~ unique(c(.x,.y)))
  vocab   <- build_vocabulary(mw_sets)
  idx     <- build_indexed_sets(mw_sets,vocab)
  
  mh <- compute_minhash_signatures(idx$sets_idx,length(vocab),
                                   num_hashes=100,seed=42)
  S <- mh$S
  
  gold <- compute_gold_duplicate_pairs(df$modelID)
  
  bands_grid <- c(5,10,20,25,50)
  thresholds <- seq(0.05,0.95,0.05)
  w_kv_grid  <- c(0.6,0.7,0.8,0.9)
  w_hsm_grid <- c(0.05,0.1,0.2)
  
  best <- list(F1=-1)
  
  for(b in bands_grid){
    cp <- lsh_candidates(S,b)
    msm_base <- NULL # only compute mw extraction once
    
    for(wkv in w_kv_grid){
      for(wh in w_hsm_grid){
        wtm <- 1 - wkv - wh
        if(wtm <= 0) next
        
        # compute MSM scores once per weight combo
        msm <- predict_msm(cp,df,title_mw,value_mw,
                           w_kv=wkv,w_hsm=wh,w_tmw=wtm)
        
        for(th in thresholds){
          pred <- msm %>% mutate(pred_duplicate = msm_score >= th)
          ev <- evaluate_final(pred,gold,N)
          
          if(ev$F1 > best$F1){
            best <- list(
              F1 = ev$F1,
              precision = ev$precision,
              recall = ev$recall,
              bands = b,
              threshold = th,
              w_kv = wkv,
              w_hsm = wh,
              w_tmw = wtm,
              eval = ev
            )
          }
        }
      }
    }
  }
  best
}


############################################################
# 9. FULL RUN
############################################################

run_pipeline_full <- function(path){
  df_raw <- load_tv_data(path)
  df <- clean_tv_data(df_raw)
  
  best <- tune_all(df)
  best
}


############################################################
# RUN 
############################################################

json_path <- "/Users/loesvanveen/Downloads/TVs-all-merged.json"

best <- run_pipeline_full(json_path)

print(best)

# FINAL REQUIRED BOOTSTRAP RUN (5 iterations)
############################################################
# BOOTSTRAP PIPELINE
############################################################

bootstrap_pipeline <- function(path,
                               B = 5,
                               num_hashes = 100,
                               bands,
                               msm_threshold,
                               w_kv = 0.8,
                               w_hsm = 0.1,
                               w_tmw = 0.1,
                               seed = 42) {
  
  set.seed(seed)
  
  # Load full dataset ONCE
  df_raw <- load_tv_data(path)
  N <- nrow(df_raw)
  
  results <- vector("list", B)
  
  for (b in seq_len(B)) {
    message("Bootstrap iteration ", b, "/", B)
    
    # ---- SAMPLE WITH REPLACEMENT ----
    train_idx <- sample(seq_len(N), N, replace = TRUE)
    test_idx  <- setdiff(seq_len(N), unique(train_idx))  # OOB samples
    
    if (length(test_idx) < 2) {
      message("Iteration ", b, " skipped (too few OOB samples)")
      next
    }
    
    # ---- OOB DATA ----
    df_sub_raw <- df_raw[test_idx, ]
    df <- clean_tv_data(df_sub_raw)
    n_sub <- nrow(df)
    
    # ---- Recompute model words ----
    title_mw <- map(df$title_clean, extract_model_words_title)
    value_mw <- map(df$features_clean, extract_model_words_values)
    mw_sets  <- map2(title_mw, value_mw, ~ unique(c(.x, .y)))
    
    # ---- Vocabulary ----
    vocab <- build_vocabulary(mw_sets)
    idx <- build_indexed_sets(mw_sets, vocab)
    
    # ---- MinHash ----
    mh <- compute_minhash_signatures(idx$sets_idx, length(vocab),
                                     num_hashes = num_hashes,
                                     seed = seed + b)
    S <- mh$S
    
    # ---- LSH ----
    cp <- lsh_candidates(S, bands = bands)
    
    # ---- Gold pairs ----
    gold <- compute_gold_duplicate_pairs(df$modelID)
    
    # ---- MSM ----
    msm <- predict_msm(cp, df, title_mw, value_mw,
                       w_kv = w_kv, w_hsm = w_hsm, w_tmw = w_tmw)
    
    pred <- msm %>% 
      mutate(pred_duplicate = msm_score >= msm_threshold)
    
    # ---- Evaluate ----
    ev <- evaluate_final(pred, gold, n_sub)
    
    results[[b]] <- ev
  }
  
  # ---- Aggregate Results ----
  F1_values  <- map_dbl(results, "F1")
  P_values   <- map_dbl(results, "precision")
  R_values   <- map_dbl(results, "recall")
  
  list(
    per_iteration = results,
    summary = list(
      F1_mean = mean(F1_values, na.rm = TRUE),
      precision_mean = mean(P_values, na.rm = TRUE),
      recall_mean = mean(R_values, na.rm = TRUE)
    )
  )
}

boot <- bootstrap_pipeline(
  path = json_path,
  B = 5,
  num_hashes = 100,
  bands = best$bands,
  msm_threshold = best$threshold,
  w_kv = best$w_kv,
  w_hsm = best$w_hsm,
  w_tmw = best$w_tmw,
  seed = 42
)

boot$summary

############################################################
# LSH-stage evaluation: PQ, PC, F1*
############################################################

evaluate_lsh_stage <- function(cp, gold){
  Nc <- nrow(cp)          # aantal candidate pairs
  Dn <- nrow(gold)        # totaal aantal echte dupes
  
  if (Nc == 0 || Dn == 0){
    return(list(PQ = 0, PC = 0, F1_star = 0, Nc = Nc, Df = 0, Dn = Dn))
  }
  
  cand_keys <- paste(cp$i, cp$j, sep = "_")
  gold_keys <- paste(gold$i, gold$j, sep = "_")
  
  Df <- sum(cand_keys %in% gold_keys)   # echte dupes binnen de candidates
  
  PQ <- Df / Nc                         # pair quality
  PC <- Df / Dn                         # pair completeness
  F1s <- ifelse(PQ + PC == 0, 0, 2 * PQ * PC / (PQ + PC))
  
  list(PQ = PQ, PC = PC, F1_star = F1s, Nc = Nc, Df = Df, Dn = Dn)
}
############################################################
# Recompute LSH + MSM for BEST config → includes PQ, PC, F1*
############################################################

recompute_for_best <- function(path, best,
                               num_hashes = 100, seed = 42){
  # 1) data + clean
  df_raw <- load_tv_data(path)
  df <- clean_tv_data(df_raw)
  N <- nrow(df)
  
  # 2) model words
  title_mw <- map(df$title_clean, extract_model_words_title)
  value_mw <- map(df$features_clean, extract_model_words_values)
  mw_sets  <- map2(title_mw, value_mw, ~ unique(c(.x, .y)))
  
  # 3) vocab + index + minhash
  vocab <- build_vocabulary(mw_sets)
  idx   <- build_indexed_sets(mw_sets, vocab)
  mh    <- compute_minhash_signatures(idx$sets_idx, length(vocab),
                                      num_hashes = num_hashes,
                                      seed = seed)
  S <- mh$S
  
  # 4) LSH with bands = best$bands
  cp   <- lsh_candidates(S, bands = best$bands)
  gold <- compute_gold_duplicate_pairs(df$modelID)
  
  # 5) MSM scores with best weights
  msm <- predict_msm(
    cp, df, title_mw, value_mw,
    w_kv  = best$w_kv,
    w_hsm = best$w_hsm,
    w_tmw = best$w_tmw
  )
  
  # 6) threshold 
  pred <- msm %>% 
    mutate(pred_duplicate = msm_score >= best$threshold)
  
  # 7) evaluations
  lsh_ev   <- evaluate_lsh_stage(cp, gold)    # PQ / PC / F1*
  final_ev <- evaluate_final(pred, gold, N)   # TP/FP/FN/TN + precision/recall/F1
  
  list(
    lsh_eval   = lsh_ev,
    final_eval = final_ev
  )
}
best_metrics <- recompute_for_best(json_path, best)

best_metrics$lsh_eval    
best_metrics$final_eval  

############################################################
# F1 vs Fraction of Comparisons
############################################################

lsh_f1_curve <- function(path,
                         num_hashes = 100,
                         bands_grid = c(2, 4, 5, 10, 20, 25, 50, 100),
                         threshold = 0.65,
                         w_kv = best$w_kv,
                         w_hsm = best$w_hsm,
                         w_tmw = best$w_tmw,
                         seed = 42) {
  # 1) Data inladen + schoonmaken
  df_raw <- load_tv_data(path)
  df <- clean_tv_data(df_raw)
  N <- nrow(df)
  total_pairs <- choose(N, 2)
  
  # 2) Model words 
  title_mw <- purrr::map(df$title_clean, extract_model_words_title)
  value_mw <- purrr::map(df$features_clean, extract_model_words_values)
  mw_sets  <- purrr::map2(title_mw, value_mw, ~ unique(c(.x, .y)))
  
  # 3) Vocab + MinHash
  vocab <- build_vocabulary(mw_sets)
  idx   <- build_indexed_sets(mw_sets, vocab)
  mh    <- compute_minhash_signatures(idx$sets_idx, length(vocab),
                                      num_hashes = num_hashes,
                                      seed = seed)
  S <- mh$S
  
  gold <- compute_gold_duplicate_pairs(df$modelID)
  
  rows <- list()
  
  # 4) Over LSH-configuraties (bands)
  for (b in bands_grid) {
    message("Bands = ", b)
    
    # LSH
    cp <- lsh_candidates(S, bands = b)
    Nc <- nrow(cp)
    fraction <- ifelse(total_pairs > 0, Nc / total_pairs, 0)
    
    if (Nc == 0) {
      rows[[length(rows) + 1]] <- tibble(
        bands = b,
        fraction = fraction,
        best_threshold = NA_real_,
        F1 = 0
      )
      next
    }
    
    # MSM-scores 
    msm <- predict_msm(cp, df, title_mw, value_mw,
                       w_kv = w_kv, w_hsm = w_hsm, w_tmw = w_tmw)
    
    # Threshold-tuning
    best_F1 <- -1
    best_th <- NA_real_
    
    
    rows[[length(rows) + 1]] <- tibble(
      bands = b,
      fraction = fraction,
      best_threshold = best_th,
      F1 = best_F1
    )
  }
  
  curve_df <- dplyr::bind_rows(rows) %>%
    dplyr::arrange(fraction)
  
  print(curve_df)
  
  # 5) Plot
  plot(curve_df$fraction, curve_df$F1,
       type = "l",
       xlab = "Fraction of comparisons",
       ylab = expression(F[1]*"-measure"),
       xlim = c(0, 1),
       ylim = c(0, 1))
  
  points(curve_df$fraction, curve_df$F1, pch = 16)
  
  curve_df
}

curve_results <- lsh_f1_curve(
  path = json_path,
  num_hashes = 100,
  bands_grid = c(2, 4, 5, 10, 20, 25, 50, 100),
  threshold = 0.65,
  w_kv = best$w_kv,
  w_hsm = best$w_hsm,
  w_tmw = best$w_tmw,
  seed = 42
)

curve_results

ggplot(curve_results, aes(x = fraction, y = F1)) +
  geom_line(size = 0.6) +              
  geom_point(size = 1.5) +             
  scale_x_continuous(
    limits = c(0, 0.10),
    breaks = seq(0, 0.10, 0.02)
  ) +
  scale_y_continuous(
    limits = c(0.37, 0.46),
    breaks = seq(0.37, 0.46, 0.02)
  ) +
  labs(
    title = "F1 vs. Fraction of Comparisons",   
    x = "Fraction of comparisons",
    y = "F1-measure"
  ) +
  theme_classic(base_size = 12) +
  theme(
    plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
    axis.text  = element_text(size = 11),
    axis.title = element_text(size = 12)
  )

############################################################
# 10. LSH-STAGE PERFORMANCE CURVES (PQ, PC, F1*)
############################################################

lsh_stage_curve <- function(path,
                            num_hashes = 100,
                            bands_grid = c(2, 4, 5, 10, 20, 25, 50, 100),
                            seed = 42) {
 
  df_raw <- load_tv_data(path)
  df <- clean_tv_data(df_raw)
  N <- nrow(df)
  total_pairs <- choose(N, 2)
  
  # 2) Model words 
  title_mw <- purrr::map(df$title_clean, extract_model_words_title)
  value_mw <- purrr::map(df$features_clean, extract_model_words_values)
  mw_sets  <- purrr::map2(title_mw, value_mw, ~ unique(c(.x, .y)))
  
  # 3) Vocab + MinHash
  vocab <- build_vocabulary(mw_sets)
  idx   <- build_indexed_sets(mw_sets, vocab)
  mh    <- compute_minhash_signatures(idx$sets_idx, length(vocab),
                                      num_hashes = num_hashes,
                                      seed = seed)
  S <- mh$S
  
  # 4) Gold pairs
  gold <- compute_gold_duplicate_pairs(df$modelID)
  
  rows <- list()
  
  # 5) Bands → blocking quality
  for (b in bands_grid) {
    message("Bands = ", b)
    
    cp <- lsh_candidates(S, bands = b)
    Nc <- nrow(cp)
    fraction <- ifelse(total_pairs > 0, Nc / total_pairs, 0)
    
    lsh_ev <- evaluate_lsh_stage(cp, gold)
    
    rows[[length(rows) + 1]] <- tibble::tibble(
      bands    = b,
      fraction = fraction,
      PQ       = lsh_ev$PQ,
      PC       = lsh_ev$PC,
      F1_star  = lsh_ev$F1_star,
      Nc       = lsh_ev$Nc,
      Df       = lsh_ev$Df,
      Dn       = lsh_ev$Dn
    )
  }
  
  dplyr::bind_rows(rows) %>%
    dplyr::arrange(fraction)
}

# Compute PQ / PC / F1* curves
curve_lsh <- lsh_stage_curve(
  path = json_path,
  num_hashes = 100,
  bands_grid = c(2, 4, 5, 10, 20, 25, 50, 100),
  seed = 42
)

curve_lsh


############################################################
# PLOTS 
############################################################

# F1* Plot
ggplot(curve_lsh, aes(x = fraction, y = F1_star)) +
  geom_line(size = 0.6) +
  geom_point(size = 1.5) +
  scale_x_continuous(limits = c(0, 0.1)) +
  scale_y_continuous(limits = c(0, 0.025)) +
  labs(
    title = expression(F[1]*"*" ~ "vs. Fraction of Comparisons"),
    x = "Fraction of comparisons",
    y = expression(F[1]*"*-measure")
  ) +
  theme_classic(base_size = 12) +
  theme(
    plot.title = element_text(size = 12, face = "bold", hjust = 0.5)
  )

# PQ Plot
ggplot(curve_lsh, aes(x = fraction, y = PQ)) +
  geom_line(size = 0.6) +
  geom_point(size = 1.5) +
  scale_x_continuous(limits = c(0, 0.1)) +
  scale_y_continuous(limits = c(0, 0.015)) +
  labs(
    title = "Pair Quality vs. Fraction of Comparisons",
    x = "Fraction of comparisons",
    y = "PQ (Pair Quality)"
  ) +
  theme_classic(base_size = 12) +
  theme(
    plot.title = element_text(size = 12, face = "bold", hjust = 0.5)
  )

# PC Plot
ggplot(curve_lsh, aes(x = fraction, y = PC)) +
  geom_line(size = 0.6) +
  geom_point(size = 1.5) +
  scale_x_continuous(limits = c(0, 0.1)) +
  scale_y_continuous(limits = c(0.4, 0.8)) +
  labs(
    title = "Pair Completeness vs. Fraction of Comparisons",
    x = "Fraction of comparisons",
    y = "PC (Pair Completeness)"
  ) +
  theme_classic(base_size = 12) +
  theme(
    plot.title = element_text(size = 12, face = "bold", hjust = 0.5)
  )

